{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "800a7478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ASUS TUFF\n",
      "[nltk_data]     GAMING\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (61.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#Import the Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!pip install wordcloud\n",
    "!pip install textblob\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fcefbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99b29d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google_play_scraper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Scrape the Data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle_play_scraper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sort, reviews_all\n\u001b[0;32m      5\u001b[0m us_reviews \u001b[38;5;241m=\u001b[39m reviews_all(\n\u001b[0;32m      6\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcom.alodokter.android\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     sleep_milliseconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     sort\u001b[38;5;241m=\u001b[39mSort\u001b[38;5;241m.\u001b[39mNEWEST\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m df_busu \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(us_reviews)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google_play_scraper'"
     ]
    }
   ],
   "source": [
    "#Scrape the Data\n",
    "#import pandas as pd\n",
    "#from google_play_scraper import Sort, reviews_all\n",
    "\n",
    "#us_reviews = reviews_all(\n",
    "#   'com.alodokter.android',\n",
    "#    sleep_milliseconds=0,\n",
    "#    lang='en',\n",
    "#    country='us',\n",
    "#    sort=Sort.NEWEST\n",
    "#)\n",
    "\n",
    "#df_busu = pd.DataFrame(us_reviews)\n",
    "#df_busu.to_csv('alodokter_reviews.csv', index=False)\n",
    "\n",
    "#print(\"Reviews have been saved to 'reviews.csv'.\")\n",
    "#Apply this to several other telemedicine startup reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37fa1b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "                                                   review sentiment\n",
      "count                                               50000     50000\n",
      "unique                                              49582         2\n",
      "top     Loved today's show!!! It was a variety and not...  positive\n",
      "freq                                                    5     25000\n"
     ]
    }
   ],
   "source": [
    "#Create the Model\n",
    "#importing the training data\n",
    "imdb_data=pd.read_csv('C:\\\\Users\\\\ASUS TUFF GAMING\\\\Downloads\\\\indonlu\\\\IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "print(imdb_data.head())\n",
    "\n",
    "#Summary of the dataset\n",
    "print(imdb_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2daa1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n"
     ]
    }
   ],
   "source": [
    "#Change it to Binary\n",
    "imdb_data['sentiment'] = imdb_data['sentiment'].replace('positive',1)\n",
    "imdb_data['sentiment'] = imdb_data['sentiment'].replace('negative',0)\n",
    "print(imdb_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df9017b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset\n",
    "#train dataset\n",
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "#test dataset\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0608bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "659e22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)\n",
    "\n",
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae17169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'having', \"shan't\", \"mustn't\", \"mightn't\", 'very', 've', 'all', 'wasn', 'with', 'or', 'from', 'm', \"doesn't\", 'isn', 'd', 'under', 'have', 'those', 'how', 'shouldn', 'yourself', 'ain', 'for', \"needn't\", \"it's\", 'up', 'couldn', 'while', 'through', 'you', 'is', 'few', \"wasn't\", 'has', 'it', 'i', 'because', 're', 'wouldn', 'were', 'into', 'at', \"you'd\", 'nor', 'her', 'his', 'my', 'off', 'am', 'we', 'no', 'below', 'themselves', \"you'll\", \"should've\", 'out', \"that'll\", 'during', 'them', 'too', 'o', \"she's\", 'only', 'herself', 'each', 'ma', 'above', 'just', \"isn't\", 'what', 'down', 'she', \"won't\", 'the', \"don't\", 'than', 'in', \"hasn't\", 'why', 'ours', \"hadn't\", 'needn', 'had', \"couldn't\", 'own', 'if', 'itself', 'weren', 'over', 'he', 'there', 'this', 'been', 'yourselves', 'when', 'before', 'do', 'now', 'are', 'once', 'about', 'ourselves', 'a', 'on', 'these', 'don', \"aren't\", 'as', 'then', 'same', 'of', 'any', 'so', 'both', 'myself', 'doing', 'again', 'further', 'll', 'doesn', \"you've\", 'most', 'yours', 'shan', 'where', \"you're\", 't', 'theirs', 'was', \"didn't\", 'mustn', 'its', 'be', 'by', 'can', 'their', 'an', \"haven't\", 'they', 'will', \"weren't\", 's', 'until', 'against', 'being', 'some', 'should', \"wouldn't\", 'hers', 'and', 'not', 'does', 'between', 'whom', 'here', 'did', 'our', 'other', 'hasn', 'mightn', 'to', 'more', 'such', 'didn', 'that', 'me', 'but', 'who', 'after', 'y', 'haven', 'won', 'aren', 'him', 'which', 'hadn', \"shouldn't\", 'himself', 'your'}\n"
     ]
    }
   ],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f73e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (40000, 6183315)\n",
      "BOW_cv_test: (10000, 6183315)\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07af179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 6183315)\n",
      "Tfidf_test: (10000, 6183315)\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f250e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54ab7a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "MultinomialNB()\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the train_sentiments array to be 1-dimensional\n",
    "train_sentiments = np.ravel(train_sentiments)\n",
    "\n",
    "# Continue with the model fitting and evaluation steps\n",
    "mnb_bow = mnb.fit(cv_train_reviews, train_sentiments)\n",
    "\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
    "print(mnb_bow)\n",
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
    "print(mnb_tfidf)\n",
    "\n",
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d538b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.7682\n",
      "mnb_tfidf_score : 0.7677\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6110fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.76      0.78      0.77      4993\n",
      "    Negative       0.77      0.76      0.77      5007\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.77      0.77      0.77     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.76      0.78      0.77      4993\n",
      "    Negative       0.77      0.76      0.77      5007\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.77      0.77      0.77     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words\n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)\n",
    "#Classification report for tfidf features\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cb9df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3803 1204]\n",
      " [1114 3879]]\n",
      "[[3791 1216]\n",
      " [1107 3886]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "514cfe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep_translator in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from deep_translator) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from deep_translator) (2.27.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus tuff gaming\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "#Translate the Reviews\n",
    "!pip install deep_translator\n",
    "from deep_translator import GoogleTranslator\n",
    "# Import the CSV\n",
    "medcall_reviews = pd.read_csv(\"C:\\\\Users\\\\ASUS TUFF GAMING\\\\PycharmProjects\\\\pythonProject4\\\\medicall_reviews.csv\")\n",
    "\n",
    "# Define the column to translate\n",
    "column_to_translate = 'content'\n",
    "\n",
    "# Translate the column\n",
    "medcall_reviews['content'] = medcall_reviews[column_to_translate].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56dd84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# Drop Unimportant Data\n",
    "columns_to_drop = ['reviewId', 'userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'replyContent', 'repliedAt','appVersion']\n",
    "medcall_reviews = medcall_reviews.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f5d5136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\ASUS TUFF\n",
      "[nltk_data]     GAMING\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download more Libraries!\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a5f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "209f1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_swn(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Map Penn Treebank POS tags to WordNet POS tags\n",
    "    tag_map = {\n",
    "        'NN': 'n',\n",
    "        'VB': 'v',\n",
    "        'JJ': 'a',\n",
    "        'RB': 'r'\n",
    "    }\n",
    "\n",
    "    # Analyze sentiment for each token\n",
    "    scores = []\n",
    "    for token, pos in pos_tags:\n",
    "        # Get the WordNet POS tag\n",
    "        wordnet_pos = tag_map.get(pos[:2], 'n')\n",
    "\n",
    "        # Lemmatize the token\n",
    "        lemma = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
    "\n",
    "        # Get the synsets for the lemma and POS tag\n",
    "        synsets = list(swn.senti_synsets(lemma, pos=wordnet_pos))\n",
    "\n",
    "        # Calculate the sentiment score\n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            senti_score = synset.pos_score() - synset.neg_score()\n",
    "            scores.append(senti_score)\n",
    "\n",
    "    # Return the average sentiment score\n",
    "    if scores:\n",
    "        return sum(scores) / len(scores)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "035f67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    return compound_score\n",
    "\n",
    "# Apply the Model to Reviews\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_score = mnb.predict(text)\n",
    "    return sentiment_score\n",
    "\n",
    "#Apply the Model to Reviews\n",
    "def analyze_sentiment_MNB(text):\n",
    "    text_vector = cv.transform([text])  # Transform the text using the CountVectorizer\n",
    "    sentiment_score = mnb.predict(text_vector)  # Predict the sentiment using the MultinomialNB classifier\n",
    "    return sentiment_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93881ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooray!\n"
     ]
    }
   ],
   "source": [
    "# Apply Lexicon Sentiment Analysis\n",
    "medcall_reviews['SentiWordNet_Score'] = medcall_reviews['content'].apply(analyze_sentiment_swn)\n",
    "medcall_reviews['VADER_Score'] = medcall_reviews['content'].apply(analyze_sentiment_vader)\n",
    "medcall_reviews['MNB Score'] = medcall_reviews['content'].apply(analyze_sentiment_MNB)\n",
    "\n",
    "medcall_reviews.to_csv(r'C:\\Users\\ASUS TUFF GAMING\\Downloads\\new_medcall.csv', index=False)\n",
    "print(\"Hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3066de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply it to Alodokter\n",
    "alodokter_reviews = pd.read_csv('C:\\\\Users\\\\ASUS TUFF GAMING\\\\PycharmProjects\\\\pythonProject4\\\\alodokter_reviews.csv')\n",
    "alodokter_reviews = alodokter_reviews.drop(columns_to_drop, axis=1)\n",
    "# Translate the column\n",
    "alodokter_reviews['content'] = alodokter_reviews[column_to_translate].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x))\n",
    "alodokter_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc02f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lexicon Sentiment Analysis\n",
    "alodokter_reviews['SentiWordNet_Score'] = alodokter_reviews['content'].apply(analyze_sentiment_swn)\n",
    "alodokter_reviews['VADER_Score'] = alodokter_reviews['content'].apply(analyze_sentiment_vader)\n",
    "alodokter_reviews['MNB Score'] = alodokter_reviews['content'].apply(analyze_sentiment_MNB)\n",
    "\n",
    "alodokter_reviews.to_csv(r'C:\\Users\\ASUS TUFF GAMING\\Downloads\\new_alodokter.csv', index=False)\n",
    "print(\"Hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply it to Halodoc\n",
    "halodoc_reviews = pd.read_csv('C:\\\\Users\\\\ASUS TUFF GAMING\\\\PycharmProjects\\\\pythonProject4\\\\halodoc_reviews.csv')\n",
    "halodoc_reviews = halodoc_reviews.drop(columns_to_drop, axis=1)\n",
    "# Translate the column\n",
    "halodoc_reviews['content'] = halodoc_reviews[column_to_translate].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x))\n",
    "halodoc_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c01973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lexicon Sentiment Analysis\n",
    "halodoc_reviews['SentiWordNet_Score'] = halodoc_reviews['content'].apply(analyze_sentiment_swn)\n",
    "halodoc_reviews['VADER_Score'] = halodoc_reviews['content'].apply(analyze_sentiment_vader)\n",
    "halodoc_reviews['MNB Score'] = halodoc_reviews['content'].apply(analyze_sentiment_MNB)\n",
    "\n",
    "halodoc_reviews.to_csv(r'C:\\Users\\ASUS TUFF GAMING\\Downloads\\new_halodoc.csv', index=False)\n",
    "print(\"Hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply it to Klikdokter\n",
    "klikdokter_reviews = pd.read_csv('C:\\\\Users\\\\ASUS TUFF GAMING\\\\PycharmProjects\\\\pythonProject4\\\\klikdokter_reviews.csv')\n",
    "klikdokter_reviews = klikdokter_reviews.drop(columns_to_drop, axis=1)\n",
    "# Translate the column\n",
    "klikdokter_reviews['content'] = klikdokter_reviews[column_to_translate].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x))\n",
    "klikdokter_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lexicon Sentiment Analysis\n",
    "klikdokter_reviews['SentiWordNet_Score'] = klikdokter_reviews['content'].apply(analyze_sentiment_swn)\n",
    "klikdokter_reviews['VADER_Score'] = klikdokter_reviews['content'].apply(analyze_sentiment_vader)\n",
    "klikdokter_reviews['MNB Score'] = klikdokter_reviews['content'].apply(analyze_sentiment_MNB)\n",
    "\n",
    "klikdokter_reviews.to_csv(r'C:\\Users\\ASUS TUFF GAMING\\Downloads\\new_klikdokter.csv', index=False)\n",
    "print(\"Hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply it to Good Doctor\n",
    "thegooddoctor_reviews = pd.read_csv('C:\\\\Users\\\\ASUS TUFF GAMING\\\\PycharmProjects\\\\pythonProject4\\\\thegooddoctor_reviews.csv')\n",
    "thegooddoctor_reviews = thegooddoctor_reviews.drop(columns_to_drop, axis=1)\n",
    "# Translate the column\n",
    "thegooddoctor_reviews['content'] = thegooddoctor_reviews[column_to_translate].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x))\n",
    "thegooddoctor_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bf3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lexicon Sentiment Analysis\n",
    "thegooddoctor_reviews['SentiWordNet_Score'] = thegooddoctor_reviews['content'].apply(analyze_sentiment_swn)\n",
    "thegooddoctor_reviews['VADER_Score'] = thegooddoctor_reviews['content'].apply(analyze_sentiment_vader)\n",
    "thegooddoctor_reviews['MNB Score'] = thegooddoctor_reviews['content'].apply(analyze_sentiment_MNB)\n",
    "\n",
    "thegooddoctor_reviews.to_csv(r'C:\\Users\\ASUS TUFF GAMING\\Downloads\\new_thegooddoctor.csv', index=False)\n",
    "print(\"Hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply it to Satu Sehat\n",
    "SATUSEHAT_reviews = pd.read_csv('C:\\\\Users\\\\ASUS TUFF GAMING\\\\PycharmProjects\\\\pythonProject4\\\\SATUSEHAT_reviews.csv')\n",
    "SATUSEHAT_reviews = SATUSEHAT_reviews.drop(columns_to_drop, axis=1)\n",
    "# Translate the column\n",
    "SATUSEHAT_reviews['content'] = SATUSEHAT_reviews[column_to_translate].apply(lambda x: GoogleTranslator(source='auto', target='en').translate(x))\n",
    "SATUSEHAT_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lexicon Sentiment Analysis\n",
    "SATUSEHAT_reviews['SentiWordNet_Score'] = thegooddoctor_reviews['content'].apply(analyze_sentiment_swn)\n",
    "SATUSEHAT_reviews['VADER_Score'] = thegooddoctor_reviews['content'].apply(analyze_sentiment_vader)\n",
    "SATUSEHAT_reviews['MNB Score'] = thegooddoctor_reviews['content'].apply(analyze_sentiment_MNB)\n",
    "\n",
    "SATUSEHAT_reviews.to_csv(r'C:\\Users\\ASUS TUFF GAMING\\Downloads\\new_SATUSEHAT.csv', index=False)\n",
    "print(\"Hooray!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
